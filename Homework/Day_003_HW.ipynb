{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業目標]\n",
    "持續接觸有關機器學習的相關專案與最新技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "透過觀察頂尖公司的機器學習文章，來了解各公司是怎麼應用機器學習在實際的專案上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業]\n",
    "今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用機器學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，這些網站都會整理最新的機器學習專案或者是技術文章，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SANPO: A Scene understanding, Accessibility, Navigation, Pathfinding, & Obstacle avoidance dataset](https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html)\n",
    "\n",
    "1. 專案的目標？\n",
    "> SANPO，這是一個用於人類以自我為中心的場景理解的大規模且具有挑戰性的視訊資料集，其中包括具有密集預測註釋的真實和合成樣本。我們希望 SANPO 能夠幫助研究人員為視障人士建立視覺導航系統，並促進視覺場景理解。\n",
    "\n",
    "2. 使用的技術是？\n",
    "> 使用AOT，一種機器學習模型，透過為註釋者提供從註釋過程中先前幀獲取的自動遮罩來開始，從而減少註釋工作。AOT 也使用手動註解的前後影格來推斷中間影格的分段註解。總體而言，這種方法減少了註釋時間，提高了邊界精度，並確保長達 30 秒的時間一致註釋。\n",
    ">\n",
    "\n",
    "3. 資料來源？\n",
    "> SANPO-Real\n",
    "SANPO-Real 是一個多視圖視訊資料集，包含使用兩個立體攝影機錄製的 701 個會話：頭戴式ZED Mini和胸部安裝式ZED-2i。也就是說，每個會話有 4 個RGB串流，幀速率為 15 FPS。597 個會話以 2208x1242 像素的分辨率錄製，其餘部分以 1920x1080 像素的分辨率錄製。每個會話長度約為 30 秒，錄製的影片均使用Zed 軟體進行校正並以無損格式儲存。每個會話都有高級屬性註釋、相機姿態軌跡、CREStereo的密集深度圖以及Zed SDK提供的稀疏深度圖。會話的子集具有時間一致性每個實例的全景分割註解。\n",
    ">\n",
    ">SANPO 包括三十種不同的類別標籤，包括各種表面（道路、人行道、路緣等）、柵欄（護欄、牆壁、大門）、障礙物（桿、自行車架、樹木）和生物（行人、騎手、動物） ）。為這些類別收集高品質的註釋是一個巨大的挑戰。為了提供時間上一致的全景分割註釋，我們使用級聯註釋協議將每個視頻劃分為 30 秒的子視頻，並每隔 5 個幀進行註釋（每個子視頻 90 幀）。在每個階段，我們要求註釋者一次圍繞五個互斥的標籤繪製邊界。我們將相同的圖像發送給不同的註釋者，其階段與收集蒙版所需的階段一樣多，直到分配所有標籤，並且先前子集的註釋被凍結並顯示給註釋者。\n",
    ">\n",
    "> 由於硬體、演算法和人為錯誤，現實世界的數據具有不完美的真實數據標籤，而合成數據具有近乎完美的真實數據並且可以自訂。我們與Parallel Domain（專門從事逼真合成資料生成的公司）合作，創建了 SANPO-Synthetic，這是一個高品質的合成資料集，以補充 SANPO-Real。Parallel Domain 擅長為機器學習應用程式創建手動合成環境和資料。由於他們的工作，SANPO-Synthetic 將現實世界的拍攝條件與相機參數、位置和場景相匹配。\n",
    "\n",
    "[the preprint](https://arxiv.org/abs/2309.12172) and on the SANPO dataset [GitHub repository](https://github.com/google-research-datasets/sanpo_dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
